{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11: Lesson: Indices, composites, correlation maps, EOF analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will introduce some tools that you might find useful for your own projects (composites, correalations).\n",
    "\n",
    "The last chapter (EOF) will introduce a new package ([eofs](http://ajdawson.github.io/eofs/)) which makes it easy to do EOS analysis in python. The EOF part is rather an example illustrating the lecture. Since it is a rather complex method, EOF analysis is not needed for your projects and there is no need to have a deep understanding of EOF analysis for the final exam either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *did* change this time! If you are on your laptop you'll have to install eofs and requests (``conda install -c conda-forge eofs requests``) before going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the plots in the notebook:\n",
    "%matplotlib inline\n",
    "# Import the tools we are going to need today:\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "import numpy as np  # numerical library\n",
    "import xarray as xr  # netCDF library\n",
    "import cartopy  # Map projections libary\n",
    "import cartopy.crs as ccrs  # Projections list\n",
    "import pandas as pd  # new package! this is the package at the base of xarray\n",
    "from eofs.xarray import Eof  # new package! http://ajdawson.github.io/eofs/index.html\n",
    "# Some defaults:\n",
    "plt.rcParams['figure.figsize'] = (12, 5)  # Default plot size\n",
    "np.set_printoptions(threshold=20)  # avoid to print very large arrays on screen\n",
    "# The commands below are to ignore all warnings, careful!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.path as mpath\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "map_circle = mpath.Path(np.vstack([np.sin(theta), np.cos(theta)]).T * 0.5 + [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some indices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python makes it very easy to read data directly from a given url:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Niño 3.4 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just reads the data from an url\n",
    "import io, requests\n",
    "# Sea Surface Temperature (SST) data from http://www.cpc.ncep.noaa.gov/data/indices/\n",
    "url = 'http://www.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/detrend.nino34.ascii.txt'\n",
    "s = requests.get(url).content\n",
    "df = pd.read_csv(io.StringIO(s.decode('utf-8')), delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panda's dataframes are one of the most widely used tool in the scientific python ecosystem. You'll get to know them better in the next semester's glaciology lecture, but for now we are going to convert this to an xarray structure, which we know better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the time and convert to xarray\n",
    "time = pd.to_datetime(df.YR.astype(str) + '-' + df.MON.astype(str))\n",
    "nino34 = xr.DataArray(df.ANOM, dims='time', coords={'time':time})\n",
    "# Apply a 3-month smoothing window\n",
    "nino34 = nino34.rolling(time=3, min_periods=3, center=True).mean()\n",
    "# Select the ERA period\n",
    "nino34 = nino34.sel(time=slice('1979', '2014'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: plot the Nino 3.4 index. Add the +0.5 and -0.5 vertical lines. Can you identify (approximately) the various Niño and Niña events? See if it fits to the list of years published [here](https://www.esrl.noaa.gov/psd/enso/past_events.html).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Atlantic Oscillation (NAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from http://www.cpc.ncep.noaa.gov/data/teledoc/nao.shtml\n",
    "url = 'http://ftp.cpc.ncep.noaa.gov/wd52dg/data/indices/nao_index.tim'\n",
    "s = requests.get(url).content\n",
    "df = pd.read_csv(io.StringIO(s.decode('utf-8')), delim_whitespace=True, skiprows=7)\n",
    "# Parse the time and convert to xarray\n",
    "time = pd.to_datetime(df.YEAR.astype(str) + '-' + df.MONTH.astype(str))\n",
    "nao = xr.DataArray(df.INDEX, dims='time', coords={'time':time})\n",
    "# Select the ERA period\n",
    "nao = nao.sel(time=slice('1979', '2014'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: plot the NAO index. How would you characterize it, in comparison to Niño 34?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically all possible climate indexes can be downloaded this way. If you want to get a specific one, let me know and I'll see if I can help you out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these simple examples, we are going to work with the good old temperature dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('./data/ERA-Int-Monthly-2mTemp.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal averages and time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can remember, groupby is a nice way to compute monthly averages for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_ma = ds.t2m.load().groupby('time.month').mean(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with groupby, you can also compute seasonal averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_sa = ds.t2m.load().groupby('time.season').mean(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: explore the t2_sa variable. What is its time stamp?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During your data exploration projects, you might find it usefull to compute seasonal timeseries. This is a little bit trickier to do, but it's possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses a series of tricks to come to the goal\n",
    "t2m_djf = ds.t2m.load().where(ds['time.season'] == 'DJF')\n",
    "t2m_djf = t2m_djf.rolling(min_periods=3, center=True, time=3).mean()\n",
    "t2m_djf = t2m_djf.groupby('time.year').mean('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are less hacky way to get to this result. But I like this method because the first year is marked as missing, an because the timestamp is how I'd like it to be.\n",
    "\n",
    "**Q: explore the data. why should the first year be missing, by the way?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El Niño / La Niña anomaly composites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E: compute the anomaly of the winter 1998 (DFJ 1998) in comparison to the 1980-2014 average. Plot it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E: by noting that ``.sel()`` also works with a list of years as argument, now compute the composite of all 10 Niño years during the ERA period. Then plot the anomaly of this composite to all other years. Repeat with La Niña composites.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nino_yrs = [1980, 1983, 1987, 1988, 1992, 1995, 1998, 2003, 2007, 2010]\n",
    "nina_yrs = [1989, 1999, 2000, 2008, 2011, 2012]\n",
    "# follow this example to compute the variables non_nino_yrs and non_nina_yrs yourself\n",
    "neutral_yrs = [yr for yr in np.arange(1980, 2015) if yr not in nino_yrs and yr not in nina_yrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind field composites, or the game of 7 differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot anomalies of the wind field as quiver plots. Let's see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data file, available on OLAT or on the scratch directory\n",
    "ds = xr.open_dataset('./data/ERA-Int-Monthly-UVSLP-TropPacific.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the seasonal averages of the wind\n",
    "u_djf = ds.u10.load().where(ds['time.season'] == 'DJF')\n",
    "u_djf = u_djf.rolling(min_periods=3, center=True, time=3).mean().groupby('time.year').mean('time')\n",
    "v_djf = ds.v10.load().where(ds['time.season'] == 'DJF')\n",
    "v_djf = v_djf.rolling(min_periods=3, center=True, time=3).mean().groupby('time.year').mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total averages\n",
    "pu = u_djf.mean(dim='year')\n",
    "pv = v_djf.mean(dim='year')\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=-180))\n",
    "pu, pv = pu[::5,::5], pv[::5,::5] \n",
    "qv = ax.quiver(pu.longitude, pu.latitude, pu.values, pv.values, transform=ccrs.PlateCarree())\n",
    "ax.coastlines(color='grey'); \n",
    "plt.title('Average surface wind DJF 1980-2014')\n",
    "qk = plt.quiverkey(qv, 0.95, 1.03, 10, '10 m s$^{-1}$', labelpos='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now repeat the operation with Niño years only\n",
    "pu = u_djf.sel(year=nino_yrs).mean(dim='year')\n",
    "pv = v_djf.sel(year=nino_yrs).mean(dim='year')\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=-180))\n",
    "pu, pv = pu[::5,::5], pv[::5,::5] \n",
    "qv = ax.quiver(pu.longitude, pu.latitude, pu.values, pv.values, transform=ccrs.PlateCarree())\n",
    "ax.coastlines(color='grey'); \n",
    "plt.title('Average surface wind DJF Niño years')\n",
    "qk = plt.quiverkey(qv, 0.95, 1.03, 10, '10 m s$^{-1}$', labelpos='W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: analyse the differences between the two plots. Hard to spot, right?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E: now compute and plot the composite anomaly of the wind field during Niño years with respect to the non Niño years average. Plot it with quiver(). Analyse.**\n",
    "\n",
    "*Note that you'll have to change the legend of the plot!*\n",
    "\n",
    "**Anomalies of the wind field are not easy to interpret! See for example the westerly anomalies of the wind at the equator: they match our conceptual model from the lecture quite well, but in reality the winds are still easterly on average, even during Niño years!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-point correlation maps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple example we seek to reproduce the one-point correlation map with SLP at Darwin, that we showed during the lecture:\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/b1njlzdwn5vr5zs/darwin_thaiti.png?dl=1\" width=\"60%\"  align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data file, available on OLAT or on the scratch directory\n",
    "ds = xr.open_dataset('./data/ERA-Int-Monthly-SLP.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the annual average\n",
    "slp = ds.msl.load().resample('AS', dim='time') / 100.\n",
    "# take the SLP at Darwin\n",
    "slp_da = slp.sel(latitude=-12.45, longitude=130, method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty array that we will fill\n",
    "cor_map = slp.isel(time=0) * 0.\n",
    "# loop over lats and lons\n",
    "for j in np.arange(len(ds.latitude)):\n",
    "    for i in np.arange(len(ds.longitude)):\n",
    "        # we use the .values attribute because this is much faster\n",
    "        cor_map.values[j, i] = np.corrcoef(slp.values[:, j, i], slp_da.values)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.Robinson(central_longitude=-180))\n",
    "cor_map.plot.contourf(ax=ax, transform=ccrs.PlateCarree(), levels=np.linspace(-0.8, 0.8, 9), \n",
    "                      extend='both', cbar_kwargs={'label':'Pearson $r$'});\n",
    "ax.coastlines(); ax.set_global();\n",
    "plt.title('Correlation coefficient of annual SLP with SLP at Darwin');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Orthogonal Functions analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will introduce a new package ([eofs](http://ajdawson.github.io/eofs/)) which makes it easy to do EOS analysis in python.Since it is a rather complex method, EOF analysis is not needed for your projects and there is no need to have a deep understanding of EOF analysis for the final exam either.\n",
    "\n",
    "EOF is a very important tool in dynamical climatology, and this is the reason why I thought you might want to know how to use it if you need it one day. We will start with a system we know well, the SST in the Tropical Pacific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data file, available on OLAT or on the scratch directory\n",
    "ds = xr.open_dataset('./data/NOAA-OI-SSTV2-TropPacific.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make seasonal means\n",
    "sst_djf = ds.sst.load().where(ds['time.season'] == 'DJF')\n",
    "sst_djf = sst_djf.rolling(min_periods=3, center=True, time=3).mean().groupby('time.year').mean('time')[1:, :, :]\n",
    "# rename the time coordinate so that eofs is happy\n",
    "sst_djf = sst_djf.rename({'year':'time'})\n",
    "# compute anomalies by removing the time-mean.\n",
    "sst_djf_a = sst_djf - sst_djf.mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grid-points need to be weighted again. Here we take the square-root\n",
    "# of cosine of latitude (this is because the weights are then squared internaly)\n",
    "wgts = sst_djf.isel(time=0) * 0. + np.sqrt(np.cos(np.deg2rad(sst_djf.lat)).clip(0., 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the EOF\n",
    "solver = Eof(sst_djf_a, weights=wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to represent the various EOFs. A widely used one is to represent the EOF as coorelation maps. The correlation values computed here are the correlations with the first (\"leading\") principal component timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the 3 first leading EOFs, expressed as the correlation between the leading\n",
    "# PC time series and the input SST anomalies at each grid point\n",
    "eofs = solver.eofsAsCorrelation(neofs=3)\n",
    "# Get the variance fraction accounted for each EOF\n",
    "variances = solver.varianceFraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the leading EOF expressed as correlation, add the percentage of explained variance in the title \n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=-180))\n",
    "eofs.sel(mode=0).plot.contourf(ax=ax, transform=ccrs.PlateCarree(), levels=np.linspace(-0.8, 0.8, 9), \n",
    "                               cmap='RdBu_r', extend='both')\n",
    "ax.coastlines(); ax.gridlines(); \n",
    "plt.title('First EOF (explained variance: %.2f)' % variances[0])\n",
    "# this is because of a cartopy bug:\n",
    "ax.set_extent([ds.lon.min(), ds.lon.max(), ds.lat.min(), ds.lat.max()], crs=ccrs.PlateCarree());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the second EOF expressed as correlation, add the percentage of explained variance in the title \n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=-180))\n",
    "eofs.sel(mode=1).plot.contourf(ax=ax, transform=ccrs.PlateCarree(), levels=np.linspace(-0.8, 0.8, 9), \n",
    "                               cmap='RdBu_r', extend='both')\n",
    "ax.coastlines(); ax.gridlines(); \n",
    "plt.title('Second EOF (explained variance: %.2f)' % variances[1])\n",
    "# this is because of a cartopy bug:\n",
    "ax.set_extent([ds.lon.min(), ds.lon.max(), ds.lat.min(), ds.lat.max()], crs=ccrs.PlateCarree());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained during the lecture, the EOFs are associated with the principal components timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = solver.pcs(npcs=3, pcscaling=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs.sel(mode=0).plot();\n",
    "plt.title('First principal component, scaled to variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs.sel(mode=1).plot();\n",
    "plt.title('Second principal component, scaled to variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By construction, the two PCs are orthogonal, their correlation is zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(pcs.sel(mode=0), pcs.sel(mode=1))[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
